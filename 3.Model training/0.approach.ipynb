{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we will fine-tune a BERT model to process stock-related news articles and obtain sentiment scores, then feed LSTM with historical stock price data, use embedding layers for stock tickers, and finally concatenate the outputs to make a prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.Preprocessing News Data for BERT: Use the processed_text field (preprocessed news) and fine-tune a BERT model to classify sentiment (positive/negative) or output a sentiment score.\n",
    "\n",
    "2.Preprocessing Stock Data for LSTM: Use stock_data (e.g., stock prices, volume) to prepare time-series data for LSTM.\n",
    "\n",
    "3.Stock Ticker Embedding: Use an embedding layer to convert stock tickers (ticker_name like WCIL.NS) into dense vectors.\n",
    "\n",
    "4.Combine Models: Concatenate the outputs of the BERT model (news sentiment) and the LSTM model (price movement) along with ticker embeddings and pass through fully connected layers for final prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Fine-tuning BERT Model on News Text\n",
    "We'll first fine-tune a BERT model to process the processed_text field (news articles) and output a sentiment score. For simplicity, we'll use a binary classification of sentiment (positive/negative) based on the finbert_analysis score, which is already provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, TFBertForSequenceClassification\n",
    "import tensorflow as tf\n",
    "\n",
    "# Load the pre-trained BERT model and tokenizer\n",
    "model_name = 'yiyanghkust/finbert-tone'  # Fine-tuned FinBERT model\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = TFBertForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "# Tokenizing the processed text (e.g., stock-related news)\n",
    "text = \"western carrier stock zoom securing r crore contract vedanta four yearlong agreement...\"\n",
    "\n",
    "inputs = tokenizer(text, return_tensors=\"tf\", truncation=True, padding=True, max_length=512)\n",
    "\n",
    "# Perform inference\n",
    "outputs = model(inputs)\n",
    "logits = outputs.logits\n",
    "predicted_class = tf.argmax(logits, axis=-1).numpy()\n",
    "\n",
    "print(f\"Predicted Sentiment Class: {predicted_class}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. LSTM Model for Stock Price Prediction\n",
    "Now, we'll use the stock price data (stock_data like avg_price_5d, volatility_5d, avg_volume_5d, etc.) and feed it into an LSTM to capture temporal dependencies.\n",
    "\n",
    "2.1 Prepare the Stock Data for LSTM\n",
    "Assuming that you're working with a window of data, you'll need to organize the historical stock data. Here's an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Example stock data for the past 5 days (just for demonstration)\n",
    "stock_data = {\n",
    "    'avg_price_5d': [114.92, 115.99, 118.50, 120.45, 121.44],\n",
    "    'volatility_5d': [0.77, 0.80, 0.75, 0.74, 0.77],\n",
    "    'avg_volume_5d': [263441.75, 250000.45, 245000.50, 255000.33, 2463672.0],\n",
    "    'open_price_news_day': [115.99, 116.20, 117.00, 118.10, 120.00],\n",
    "    'close_price_news_day': [121.44, 119.50, 118.80, 121.00, 122.50]\n",
    "}\n",
    "\n",
    "# Convert into pandas DataFrame\n",
    "df = pd.DataFrame(stock_data)\n",
    "\n",
    "# Normalize the stock data using MinMaxScaler\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scaled_data = scaler.fit_transform(df.values)\n",
    "\n",
    "# Reshape the data into sequences for LSTM input\n",
    "lookback_window = 5  # Use the last 5 days of data to predict\n",
    "\n",
    "X = []\n",
    "y = []\n",
    "\n",
    "for i in range(len(scaled_data) - lookback_window):\n",
    "    X.append(scaled_data[i:i+lookback_window])\n",
    "    y.append(scaled_data[i+lookback_window][3])  # Use close price as target for prediction\n",
    "\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "\n",
    "# Split data into training and testing sets\n",
    "train_size = int(len(X) * 0.8)\n",
    "X_train, X_test = X[:train_size], X[train_size:]\n",
    "y_train, y_test = y[:train_size], y[train_size:]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.2 Define the LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "\n",
    "# Define the LSTM model for stock price prediction\n",
    "lstm_model = Sequential()\n",
    "lstm_model.add(LSTM(units=50, return_sequences=True, input_shape=(X_train.shape[1], X_train.shape[2])))\n",
    "lstm_model.add(Dropout(0.2))\n",
    "lstm_model.add(LSTM(units=50, return_sequences=False))\n",
    "lstm_model.add(Dropout(0.2))\n",
    "lstm_model.add(Dense(units=1))\n",
    "\n",
    "lstm_model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Train the model\n",
    "lstm_model.fit(X_train, y_train, epochs=10, batch_size=32)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Embedding Layers for Stock Tickers\n",
    "Next, use an embedding layer to process stock tickers (WCIL.NS). This transforms the categorical stock ticker into a continuous embedding vector.\n",
    "\n",
    "3.1 Define the Embedding Layer for Stock Ticker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Embedding, Input, Flatten\n",
    "\n",
    "# Example: Use stock tickers as an input and create embeddings for them\n",
    "ticker_input = Input(shape=(1,), dtype=tf.int32, name=\"ticker\")\n",
    "embedding = Embedding(input_dim=9000, output_dim=50)(ticker_input)  # 9,000 tickers and embedding dimension of 50\n",
    "ticker_embedding = Flatten()(embedding)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Final Model: Combining BERT, LSTM, and Ticker Embeddings\n",
    "Finally, concatenate the outputs from BERT (news sentiment), LSTM (stock price movement), and ticker embeddings, then pass them through a fully connected layer for final prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import concatenate\n",
    "\n",
    "# Define the inputs for BERT, LSTM, and stock ticker embedding\n",
    "bert_input = Input(shape=(512,), dtype=tf.int32, name=\"bert_input\")  # For BERT tokenized input\n",
    "lstm_input = Input(shape=(X_train.shape[1], X_train.shape[2]), name=\"lstm_input\")  # For LSTM time-series data\n",
    "ticker_input = Input(shape=(1,), dtype=tf.int32, name=\"ticker_input\")  # For Stock Ticker embedding\n",
    "\n",
    "# BERT Model for sentiment (already fine-tuned)\n",
    "bert_model = TFBertForSequenceClassification.from_pretrained(model_name)\n",
    "bert_output = bert_model(bert_input).logits\n",
    "bert_output = tf.keras.layers.Dense(1, activation=\"sigmoid\")(bert_output)\n",
    "\n",
    "# LSTM Model for stock price prediction (already defined)\n",
    "lstm_output = lstm_model(lstm_input)\n",
    "\n",
    "# Ticker Embedding Layer (already defined)\n",
    "ticker_embedding_output = ticker_embedding\n",
    "\n",
    "# Concatenate the outputs of all models\n",
    "merged = concatenate([bert_output, lstm_output, ticker_embedding_output])\n",
    "\n",
    "# Fully connected layers for final prediction\n",
    "fc = Dense(128, activation=\"relu\")(merged)\n",
    "fc = Dropout(0.5)(fc)\n",
    "final_output = Dense(1, activation=\"sigmoid\")(fc)  # Binary classification (up/down)\n",
    "\n",
    "# Final Model\n",
    "final_model = tf.keras.Model(inputs=[bert_input, lstm_input, ticker_input], outputs=final_output)\n",
    "\n",
    "# Compile the model\n",
    "final_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Model summary\n",
    "final_model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Train the Final Model\n",
    "Now, you can train the final model with the combined data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assume X_bert_input, X_lstm_input, and X_ticker_input are your inputs\n",
    "final_model.fit([X_bert_input, X_lstm_input, X_ticker_input], y_train, epochs=10, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summary of the Model Architecture\n",
    "BERT: Fine-tuned for sentiment analysis of stock-related news (processed text).\n",
    "LSTM: Processes historical stock prices to capture temporal relationships.\n",
    "Stock Ticker Embedding: Efficient representation of stock tickers as embeddings.\n",
    "Fully Connected Layers: Combine the outputs from BERT, LSTM, and stock ticker embeddings to make the final prediction.\n",
    "This model is designed to handle both news data (processed text) and time-series stock data, combining them into a comprehensive model for stock market prediction."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
