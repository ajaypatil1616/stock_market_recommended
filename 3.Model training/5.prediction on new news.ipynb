{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from bs4 import BeautifulSoup\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def finding_raw_urls_from_base(url):\n",
    "    service = Service(\"./chromedriver/chromedriver.exe\")\n",
    "    options = Options()\n",
    "    options.add_argument(\"--headless\")\n",
    "    options.add_argument(\"--disable-gpu\")\n",
    "    options.add_argument(\"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\")\n",
    "    \n",
    "    driver = webdriver.Chrome(service=service, options=options)\n",
    "    driver.get(url)\n",
    "    \n",
    "    WebDriverWait(driver, 60).until(\n",
    "        EC.presence_of_all_elements_located((By.TAG_NAME, 'a'))\n",
    "    )\n",
    "    \n",
    "    html = driver.page_source\n",
    "    driver.quit()\n",
    "    \n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    links = soup.find_all('a', href=True)\n",
    "    raw_urls = []\n",
    "    for link in links:\n",
    "        if link['href'].startswith(\"https://www.moneycontrol.com/news/business/\"):\n",
    "            raw_urls.append(link['href'])\n",
    "    return list(set(raw_urls))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_urls = finding_raw_urls_from_base(\"https://www.moneycontrol.com/news/business/stocks/\")\n",
    "# raw_urls = finding_raw_urls_from_base(\"https://www.moneycontrol.com/news/business/stocks/page-3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_urls =[]\n",
    "market_urls = []\n",
    "next_page_urls = []\n",
    "other_urls = []\n",
    "ipo_urls = []\n",
    "def extract_urls(all_urls):\n",
    "    for link in raw_urls:\n",
    "        if link.startswith(\"https://www.moneycontrol.com/news/business/stocks/page\"):\n",
    "            next_page_urls.append(link)\n",
    "        elif link.startswith(\"https://www.moneycontrol.com/news/business/stocks/\"):\n",
    "            stock_urls.append(link)\n",
    "        elif link.startswith(\"https://www.moneycontrol.com/news/business/markets/\"):\n",
    "            market_urls.append(link)\n",
    "        elif link.startswith(\"https://www.moneycontrol.com/news/business/ipo/\"):\n",
    "            ipo_urls.append(link)\n",
    "        else:\n",
    "            other_urls.append(link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_urls(raw_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "market_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "regex = r'https:\\/\\/www\\.moneycontrol\\.com\\/news\\/business\\/stocks\\/[^\\/\\s]+(?:\\.[a-z]{2,6})(?:[\\/\\?].*)?'\n",
    "\n",
    "final_stocks_urls = [url for url in stock_urls if re.match(regex, url)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_stocks_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "service = Service(\"./chromedriver/chromedriver.exe\")\n",
    "options = Options()\n",
    "options.add_argument(\"--headless\")\n",
    "options.add_argument(\"--disable-gpu\")\n",
    "options.add_argument(\"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.5735.199 Safari/537.36\")\n",
    "\n",
    "driver = webdriver.Chrome(service=service, options= options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_data(url):  \n",
    "    driver.get(url)\n",
    "    \n",
    "    WebDriverWait(driver, 60).until(\n",
    "        lambda d: d.execute_script(\"return document.readyState\") == \"complete\"\n",
    "    )\n",
    "    \n",
    "    html = driver.page_source\n",
    "    soup = BeautifulSoup(html, 'html.parser')    \n",
    "    news = {}\n",
    "    \n",
    "    title = soup.find('h1', class_ = \"article_title\")\n",
    "    news.update({\"title\":f\"{title.text.strip()}\"})\n",
    "    \n",
    "    desc = soup.find('h2', class_ = 'article_desc')\n",
    "    news.update({\"desc\":f\"{desc.text.strip()}\"})\n",
    "    \n",
    "    date_time_div = soup.find('div', class_=\"article_schedule\")\n",
    "    if date_time_div:\n",
    "        span_tag = date_time_div.find('span')\n",
    "        date = span_tag.text.strip()\n",
    "        news.update({\"date\":f\"{date}\"})\n",
    "    \n",
    "    datetime = date_time_div.text.strip()\n",
    "    news.update({\"datetime\":f\"{datetime}\"})\n",
    "    \n",
    "    paragraphs_list = []\n",
    "    paragrphs_div = soup.find('div', class_ =\"content_wrapper\")\n",
    "    if paragrphs_div:\n",
    "        paragraph_tags = paragrphs_div.find_all('p')\n",
    "        for p in paragraph_tags:\n",
    "            para_text = p.text.strip()            \n",
    "            if len(para_text) < 50 :\n",
    "                continue            \n",
    "            if re.search(r\"(click\\s+here|disclaimer|modal|window|advertisement|investment\\s+tips)\", para_text, re.IGNORECASE):\n",
    "                continue            \n",
    "            paragraphs_list.append(para_text)         \n",
    "            \n",
    "    news.update({\"content\": paragraphs_list})\n",
    "    \n",
    "    stock_name = soup.find('a', class_=\"stock-name\")\n",
    "    if stock_name:\n",
    "        news.update({\"stock_name\":f\"{stock_name.text.strip()}\"})\n",
    "            \n",
    "    # driver.quit()   \n",
    "    return news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_news_data = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for url in final_stocks_urls :\n",
    "    data = extract_data(url)\n",
    "    raw_news_data.append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.quit()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# raw_news_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Processed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "# import spacy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_news(news):\n",
    "    text = f\"{news['title']} {news['desc']} {' '.join(news['content'])}\"\n",
    "    \n",
    "    text = re.sub(r\"[^a-zA-Z\\s]\", \"\", text)\n",
    "    text = re.sub(r\"\\s+\",\" \", text).strip()\n",
    "    \n",
    "    text = text.lower()\n",
    "    \n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    \n",
    "    lemmatized_token = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    return \" \".join(lemmatized_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_date(date):\n",
    "    date_obj = datetime.strptime(date, \"%B %d, %Y\") # strptime str -> datetime obj\n",
    "    formatted_date = date_obj.strftime(\"%Y-%m-%d\")  # strftime  datetime obj -> formatted str\n",
    "    return formatted_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_datetime(raw_datetime):\n",
    "    raw_datetime_split = raw_datetime.split(\"/\")\n",
    "    date_obj = datetime.strptime(raw_datetime_split[0].strip(), \"%B %d, %Y\")\n",
    "    formatted_date = date_obj.strftime(\"%Y-%m-%d\")\n",
    "    time_obj =  raw_datetime_split[1].strip().replace(\"IST\",\"\")\n",
    "    formatted_datetime = formatted_date + \" \" + time_obj\n",
    "    return formatted_datetime.strip()\n",
    "\n",
    "# print(preprocess_datetime(\"January 02, 2025 / 17:53 IST\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from yahooquery import search\n",
    "\n",
    "def extract_ticker_name(company_name):\n",
    "    company_name = company_name.strip()\n",
    "    ticker_name = None       \n",
    "    if company_name:\n",
    "        results = search(company_name)        \n",
    "        if results and 'quotes' in results:\n",
    "            for quote in results['quotes']:\n",
    "                if 'symbol' in quote and 'longname' in quote:\n",
    "                    if company_name.lower() in quote['longname'].lower():\n",
    "                        ticker_name = quote['symbol']\n",
    "                        break      \n",
    "    return ticker_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_complete_raw_news(news):\n",
    "    text = f\"{news['title']} {news['desc']} {' '.join(news['content'])}\"\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_data = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for news in raw_news_data:\n",
    "    processed_text = preprocess_news(news)\n",
    "    formatted_date = preprocess_date(news['date'])\n",
    "    formatted_datetime = preprocess_datetime(news['datetime'])\n",
    "    raw_news = get_complete_raw_news(news)\n",
    "    if \"stock_name\" in news:\n",
    "        ticker_name = extract_ticker_name(news[\"stock_name\"]) \n",
    "    \n",
    "        processed_data.append({\n",
    "            # \"_id\": news[\"_id\"],\n",
    "            \"raw_news\":raw_news,\n",
    "            \"processed_text\": processed_text,\n",
    "            \"date\": formatted_date,\n",
    "            \"datetime\": formatted_datetime,\n",
    "            \"stock_name\": news.get(\"stock_name\"),\n",
    "            \"ticker_name\":ticker_name,        \n",
    "        })\n",
    "    else:\n",
    "        processed_data.append({\n",
    "                # \"_id\": news[\"_id\"],\n",
    "                \"raw_news\":raw_news,\n",
    "                \"processed_text\": processed_text,\n",
    "                \"date\": formatted_date,\n",
    "                \"datetime\": formatted_datetime,\n",
    "                \"stock_name\": news.get(\"stock_name\"),      \n",
    "                \"ticker_name\": None,      \n",
    "            })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# processed_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Manually updating bad records "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for processed_news in processed_data:\n",
    "    if not processed_news['stock_name']:\n",
    "        stock_name = input(f\"Enter stock name {processed_news['raw_news']}\")\n",
    "        processed_news.update({\"stock_name\":f\"{stock_name}\"})\n",
    "    if not processed_news['ticker_name']:\n",
    "        ticker_name = input(f\"Enter ticker name for {processed_news['raw_news']}\")\n",
    "        processed_news.update({\"ticker_name\":f\"{ticker_name}\"})\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"yiyanghkust/finbert-tone\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"yiyanghkust/finbert-tone\")\n",
    "nlp_pipeline = pipeline(\"sentiment-analysis\", model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_text(text, max_length=500):\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    chunks = [tokens[i:i+max_length] for i in range(0, len(tokens), max_length)]\n",
    "    return [tokenizer.convert_tokens_to_string(chunk) for chunk in chunks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for news in processed_data:\n",
    "    \n",
    "    text = news['raw_news']\n",
    "    \n",
    "    text_chunks = split_text(text)\n",
    "    sentiment_results = []\n",
    "    \n",
    "    for chunk in text_chunks:\n",
    "        sentiment_result = nlp_pipeline(chunk)\n",
    "        sentiment_results.extend(sentiment_result)\n",
    "\n",
    "    i = 0\n",
    "    while i < len(sentiment_results) - 1:\n",
    "        if sentiment_results[i]['label'] == sentiment_results[i + 1]['label']:\n",
    "            avg_score = (sentiment_results[i]['score'] + sentiment_results[i + 1]['score']) / 2\n",
    "            sentiment_results[i] = {\"label\": sentiment_results[i]['label'], \"score\": avg_score}\n",
    "            del sentiment_results[i + 1]  \n",
    "        else:\n",
    "            i += 1  \n",
    "    news.update({\"finbert_analysis\":sentiment_results})\n",
    " \n",
    "    # print(\"Updated Sentiment Results:\", sentiment_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finbert_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for data in processed_data:\n",
    "    if len(data['finbert_analysis']) == 1:\n",
    "        if data['stock_name'] and data['ticker_name']:\n",
    "            finbert_list.append(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finbert_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "def get_stock_features(ticker, news_date):\n",
    "    \n",
    "    try:\n",
    "        news_date = datetime.strptime(news_date, \"%Y-%m-%d\")\n",
    "        before_start = (news_date - timedelta(days=5)).strftime(\"%Y-%m-%d\")\n",
    "        after_end = (news_date + timedelta(days=5)).strftime(\"%Y-%m-%d\")\n",
    "        \n",
    "        \n",
    "        stock = yf.Ticker(ticker)\n",
    "        try:\n",
    "            historical_data = stock.history(start=before_start, end=after_end)  \n",
    "            historical_data.index = historical_data.index.tz_localize(None)\n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching data for ticker {ticker}: {e}\")\n",
    "            return None\n",
    "            \n",
    "            \n",
    "        if historical_data.empty:\n",
    "            return None\n",
    "        \n",
    "        #before news stock data\n",
    "        before_news = historical_data.loc[:news_date - timedelta(days=1)]\n",
    "        avg_price_5d = before_news['Close'].mean()\n",
    "        vol_5d = before_news['Close'].std()\n",
    "        avg_volume_5d = before_news['Volume'].mean()\n",
    "        \n",
    "        # news day stock data\n",
    "        if news_date.strftime(\"%Y-%m-%d\") in historical_data.index:\n",
    "            news_day = historical_data.loc[news_date.strftime(\"%Y-%m-%d\")]\n",
    "            open_price = news_day['Open']\n",
    "            close_price = news_day['Close']\n",
    "            volume_news_day = news_day['Volume']\n",
    "            stock_movement = \"up\" if close_price > open_price else \"down\"\n",
    "        else:\n",
    "            open_price = close_price = volume_news_day = None\n",
    "        \n",
    "        # gap\n",
    "        prev_day_idx = historical_data.index.get_loc(news_date) - 1\n",
    "        if prev_day_idx >= 0:\n",
    "            prev_close = historical_data.iloc[prev_day_idx]['Close']\n",
    "            gap = open_price - prev_close if open_price is not None else None\n",
    "        else:\n",
    "            prev_close = None\n",
    "            gap = None\n",
    "        \n",
    "        # after news stock data \n",
    "        after_news = historical_data.loc[news_date + timedelta(days=1):]\n",
    "        price_movement_1d = ((after_news['Close'].iloc[0] - open_price) / open_price) * 100 if len(after_news) > 0 and open_price else None\n",
    "        price_movement_3d = ((after_news['Close'].iloc[2] - open_price) / open_price) * 100 if len(after_news) > 2 and open_price else None\n",
    "        price_movement_5d = ((after_news['Close'].iloc[4] - open_price) / open_price) * 100 if len(after_news) > 4 and open_price else None\n",
    "        # avg_volume_after = after_news['Volume'].mean() if len(after_news) > 0 else None\n",
    "        \n",
    "        return {\n",
    "            \"avg_price_5d\": avg_price_5d,\n",
    "            \"volatility_5d\": vol_5d,\n",
    "            \"avg_volume_5d\": avg_volume_5d,\n",
    "            \"open_price_news_day\": open_price,\n",
    "            \"close_price_news_day\": close_price,\n",
    "            \"volume_news_day\": volume_news_day,\n",
    "            \"price_movement_1d\": price_movement_1d,\n",
    "            \"price_movement_3d\": price_movement_3d,\n",
    "            # \"price_movement_5d\": price_movement_5d,\n",
    "            # \"avg_volume_after\": avg_volume_after,\n",
    "            \"gap\":gap,\n",
    "            # \"stock_movement_on_news_day\":stock_movement,\n",
    "        }\n",
    "    except Exception as e :\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0 \n",
    "for finbert_added_data in finbert_list:\n",
    "    ticker = finbert_added_data['ticker_name'].strip()\n",
    "    date = finbert_added_data['date'].strip()\n",
    "    \n",
    "    data = get_stock_features(ticker, date)\n",
    "    print(count)\n",
    "    finbert_added_data.update({\"stock_data\": data})\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finbert_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "presentation_data_df = pd.DataFrame(finbert_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_column_df = presentation_data_df[['finbert_analysis','stock_data']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extraction_finbert_data(finbert_list_dict):\n",
    "    label = finbert_list_dict[0].get('label')    \n",
    "    score = finbert_list_dict[0].get('score')\n",
    "    return label, score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_column_df[['finbert_label', 'finbert_score']] = selected_column_df['finbert_analysis'].apply(extraction_finbert_data).apply(pd.Series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_stock_data(stock_data):\n",
    "    avg_price_5d = stock_data.get('avg_price_5d')\n",
    "    volatility_5d = stock_data.get('volatility_5d')\n",
    "    avg_volume_5d = stock_data.get('avg_volume_5d')\n",
    "    open_price_news_day = stock_data.get('open_price_news_day')\n",
    "    close_price_news_day = stock_data.get('close_price_news_day')\n",
    "    volume_news_day = stock_data.get('volume_news_day')\n",
    "    price_movement_1d = stock_data.get('price_movement_1d')\n",
    "    gap = stock_data.get('gap')\n",
    "    stock_movement_on_news_day = stock_data.get('stock_movement_on_news_day')\n",
    "    return avg_price_5d,volatility_5d, avg_volume_5d, open_price_news_day, close_price_news_day, volume_news_day, price_movement_1d,gap, stock_movement_on_news_day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_column_df[['avg_price_5d','volatility_5d','avg_volume_5d','open_price_news_day','close_price_news_day','volume_news_day','price_movement_1d','gap','stock_movement_on_news_day']] =selected_column_df['stock_data'].apply(extract_stock_data).apply(pd.Series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropped_column_df = selected_column_df.drop(columns=['finbert_analysis','stock_data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_label_to_numeric(label):\n",
    "    if label.strip() == 'Positive':\n",
    "        return 1\n",
    "    elif label.strip() == \"Neutral\":\n",
    "        return 0\n",
    "    elif label.strip() == \"Negative\":\n",
    "        return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropped_column_df['finbert_label'] = dropped_column_df['finbert_label'].apply(convert_label_to_numeric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_target_varible(output):\n",
    "    if output.strip() == \"up\":\n",
    "        return 1\n",
    "    elif output.strip() == \"down\":\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropped_column_df['stock_movement_on_news_day'] = dropped_column_df['stock_movement_on_news_day'].apply(convert_target_varible)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def round_out_value(value):\n",
    "    try:\n",
    "        return round(value, 3)\n",
    "    except:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropped_column_df['finbert_score'] = dropped_column_df['finbert_score'].apply(round_out_value)\n",
    "dropped_column_df['avg_price_5d'] = dropped_column_df['avg_price_5d'].apply(round_out_value)\n",
    "dropped_column_df['volatility_5d'] = dropped_column_df['volatility_5d'].apply(round_out_value)\n",
    "dropped_column_df['avg_volume_5d'] = dropped_column_df['avg_volume_5d'].apply(round_out_value)\n",
    "dropped_column_df['open_price_news_day'] = dropped_column_df['open_price_news_day'].apply(round_out_value)\n",
    "dropped_column_df['close_price_news_day'] = dropped_column_df['close_price_news_day'].apply(round_out_value)\n",
    "dropped_column_df['volume_news_day'] = dropped_column_df['volume_news_day'].apply(round_out_value)\n",
    "dropped_column_df['price_movement_1d'] = dropped_column_df['price_movement_1d'].apply(round_out_value)\n",
    "dropped_column_df['gap'] = dropped_column_df['gap'].apply(round_out_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropped_column_df = dropped_column_df.fillna(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dropped_column_df =dropped_column_df.drop(columns='stock_movement_on_news_day')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dill\n",
    "with open(\"./Stock_prediction_model.pkl\", \"rb\") as file:\n",
    "    loaded_model = dill.load(file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(dropped_column_df)):\n",
    "    row_dict = dropped_column_df.iloc[i].to_dict()\n",
    "    new_data_df = pd.DataFrame([row_dict])\n",
    "    predicted_movement = loaded_model.predict(new_data_df)\n",
    "    print(\"Predicted Stock Movement:\", \"Up\" if predicted_movement[0] == 1 else \"Down\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
