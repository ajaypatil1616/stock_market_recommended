{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_ticker_list = ['SBIN.NS',\n",
    "                     'COALINDIA.NS',\n",
    "                     'MOIL.NS'\n",
    "#     'WCIL.NS',\n",
    "#  'CELLO.NS',\n",
    "#  'PRICOLLTD.NS',\n",
    "#  'SHIVATEX.NS',\n",
    "#  'EICHERMOT.NS',\n",
    "#  'RITESHIN.BO',\n",
    "#  'GEOJITFSL.NS',\n",
    "#  'RAILTEL.NS',\n",
    "#  'SIGNATURE.NS',\n",
    "#  'SKYGOLD.NS',\n",
    "#  'GOACARBON.BO',\n",
    "#  'LT.NS',\n",
    "#  'SUNPHARMA.NS',\n",
    "#  'PGIL.NS',\n",
    "#  'APLAPOLLO.NS',\n",
    "#  'KPIL.NS',\n",
    "#  'POLYCAB.NS',\n",
    "#  'METROBRAND.BO',\n",
    "#  'LUPIN.NS',\n",
    "#  'SHRIRAMPPS.NS',\n",
    "#  'JUBLFOOD.NS',\n",
    "#  'ITC.NS',\n",
    "#  'WAAREEENER.BO',\n",
    "#  'ICICIPRULI.BO',\n",
    "#  'SUZLON.NS',\n",
    "#  'IPCALAB.NS',\n",
    "#  'KEI.NS',\n",
    "#  'WELSPUNLIV.NS',\n",
    "#  'JSWENERGY.NS',\n",
    "#  'MAZDOCK.BO',\n",
    "#  'HOMEFIRST.NS',\n",
    "#  'JSWINFRA.BO',\n",
    "#  'PETRONET.NS',\n",
    "#  'INDUSINDBK.NS',\n",
    "#  'EPACK.NS',\n",
    "#  'RAYMONDLSL.NS',\n",
    "#  'HDFCBANK.NS',\n",
    "#  'RAYMONDLSL.NS',\n",
    "#  'PANACEABIO.NS',\n",
    "#  'BPCL.NS',\n",
    "#  'RAMKY.BO',\n",
    "#  'HGINFRA.NS',\n",
    "#  'VEDL.NS',\n",
    "#  'LT.NS',\n",
    "#  'HGINFRA.NS',\n",
    "#  'HAL.NS',\n",
    "#  'PNBHOUSING.BO',\n",
    "#  'HDFCBANK.NS',\n",
    "#  'NTPCGREEN.BO',\n",
    "#  'AUROPHARMA.NS',\n",
    "#  'IBN',\n",
    "#  'AVANTIFEED.NS',\n",
    "#  'ANANTRAJ.BO',\n",
    "#  'DEVYANI.BO',\n",
    "#  'JSWINFRA.BO',\n",
    "#  'TATAMOTORS.NS',\n",
    "#  'SPICEJET.BO',\n",
    "#  'MTARTECH.NS',\n",
    "#  'ZODIAC.NS',\n",
    "#  'LUMAXTECH.BO',\n",
    "#  'SPICEJET.BO',\n",
    "#  'MTARTECH.NS',\n",
    "#  'ZODIAC.NS',\n",
    "#  'LUMAXTECH.BO',\n",
    "#  'BHARTIHEXA.NS',\n",
    "#  'PIIND.BO',\n",
    "#  'STOVEKRAFT.BO',\n",
    "#  'SWIGGY.NS',\n",
    "#  'GODREJIND.BO',\n",
    "#  'RDY',\n",
    "#  'AWFIS.NS',\n",
    "#  'AUROPHARMA.NS',\n",
    "#  'WABAG.NS',\n",
    "#  'SBIN.NS',\n",
    "#  'COALINDIA.NS',\n",
    "#  'EXIDEIND.BO',\n",
    "#  'DIXON.BO',\n",
    "#  'ASHOKLEY.NS',\n",
    "#  'NCC.NS',\n",
    "#  'TEXRAIL.BO',\n",
    "#  'TATAPOWER.NS',\n",
    "#  'CELLO.BO',\n",
    "#  'BANKBETF.NS',\n",
    "#  'POLYCAB.NS',\n",
    "#  'PERSISTENT.NS',\n",
    "#  'AFCONS.BO',\n",
    "#  'PREMEXPLN.NS',\n",
    "#  'CRISIL.NS',\n",
    "#  'BANKBETF.NS',\n",
    "#  'ASHOKLEY.NS',\n",
    "#  'IBN',\n",
    "#  'JSFB.NS',\n",
    "#  'BAJFINANCE.NS',\n",
    "#  'ASTRAMICRO.NS',\n",
    "#  'CEATLTD.NS',\n",
    "#  'POWERMECH.BO',\n",
    "#  'TRIVENI.NS',\n",
    "#  'CEATLTD.NS',\n",
    "#  'BAJFINANCE.NS',\n",
    "#  'AUBANK.BO',\n",
    "#  'METROPOLIS.NS',\n",
    "#  'ACMESOLAR.NS',\n",
    "#  'SHRIPISTON.NS',\n",
    "#  'GODREJCP.BO',\n",
    "#  'BEL.NS',\n",
    "#  'GODREJCP.BO',\n",
    "#  'GLAND.BO',\n",
    "#  'HEROMOTOCO.NS',\n",
    "#  'NTPCGREEN.BO',\n",
    "#  'BAJFINANCE.NS',\n",
    "#  'DELHIVERY.BO',\n",
    "#  'BAJFINANCE.NS',\n",
    "#  'HEROMOTOCO.NS',\n",
    "#  'VOLTAS.NS',\n",
    "#  'CANFINHOME.NS',\n",
    "#  'METROPOLIS.NS',\n",
    "#  'CERA.NS',\n",
    "#  'MANAPPURAM.NS',\n",
    "#  'GOKEX.BO',\n",
    "#  'HEROMOTOCO.NS',\n",
    "#  'BEL.NS',\n",
    "#  'INFY',\n",
    "#  'TATAPOWER.NS',\n",
    "#  'HEROMOTOCO.NS',\n",
    "#  'TATAPOWER.NS',\n",
    "#  'EUREKAFORB.NS',\n",
    "#  'SUPREMEIND.NS',\n",
    "#  'HCLTECH.NS',\n",
    "#  'KPITTECH.NS',\n",
    "#  'SONATSOFTW.NS',\n",
    "#  'HDFCBANK.NS',\n",
    "#  'JYOTHYLAB.NS',\n",
    "#  'SURAJEST.NS',\n",
    "#  'EIHOTEL.NS',\n",
    "#  'TORNTPHARM.NS',\n",
    "#  'IWEL.NS',\n",
    "#  'LEMONTREE.BO',\n",
    "#  'M&M.NS',\n",
    "#  'HAL.NS',\n",
    "#  'NETWEB.NS',\n",
    "#  'RVNL.NS',\n",
    "#  'HGINFRA.NS',\n",
    "#  'DIXON.BO',\n",
    "#  'CIPLA.NS',\n",
    "#  'DIXON.BO',\n",
    "#  'HAL.NS',\n",
    "#  'M&M.NS',\n",
    "#  'HGINFRA.NS',\n",
    "#  'NETWEB.NS',\n",
    "#  'HATSUN.NS',\n",
    "#  'RBLBANK.NS',\n",
    "#  '000751.SZ',\n",
    "#  'RVNL.NS',\n",
    "#  'FLUOROCHEM.BO',\n",
    "#  'PRICOLLTD.NS',\n",
    "#  'SOLARINDS.BO',\n",
    "#  'BAJFINANCE.NS',\n",
    "#  'SOLARINDS.BO',\n",
    "#  'ANANTRAJ.BO',\n",
    "#  'RBLBANK.NS',\n",
    "#  'KPIGREEN.NS',\n",
    "#  'RBLBANK.NS',\n",
    "#  '000751.SZ',\n",
    "#  'BAJFINANCE.NS',\n",
    "#  '000751.SZ',\n",
    "#  'BRIGADE.NS',\n",
    "#  'NYKAA.NS',\n",
    "#  'NTPCGREEN.BO',\n",
    "#  'ITC.NS',\n",
    "#  'SHIVATEX.NS',\n",
    "#  'RADICO.NS',\n",
    "#  'WCIL.NS',\n",
    "#  'COALINDIA.NS',\n",
    "#  'CELLO.NS',\n",
    "#  'SIGNATURE.NS',\n",
    "#  'PRICOLLTD.NS',\n",
    "#  'RITESHIN.BO',\n",
    "#  'EICHERMOT.NS',\n",
    "#  'RADICO.NS'\n",
    " ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Embedding, Input, Concatenate, Dropout, Flatten\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import yfinance as yf\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "lookback = 60\n",
    "feature_columns = ['Open', 'High', 'Low', 'Close', 'Volume']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n"
     ]
    }
   ],
   "source": [
    "def fetch_data(ticker_list):\n",
    "    all_data = []\n",
    "    for ticker in ticker_list:\n",
    "        try:\n",
    "            stock_data = yf.download(ticker, period=\"max\")\n",
    "            first_date = stock_data.index.min()\n",
    "    \n",
    "            ideal_start_date = pd.to_datetime('2010-01-01')\n",
    "            start_date = ideal_start_date if first_date < ideal_start_date else first_date\n",
    "            \n",
    "            data = yf.download(ticker, start=start_date, end=\"2025-01-01\")\n",
    "            data['ticker'] = ticker\n",
    "            all_data.append(data)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching {ticker}: {e}\")\n",
    "    \n",
    "    \n",
    "    all_data_df = pd.concat(all_data)\n",
    "    \n",
    "    # Reseting  index to make the date a column in the DataFrame\n",
    "    all_data_df.reset_index(inplace=True)\n",
    "    \n",
    "    return all_data_df\n",
    "\n",
    "\n",
    "ticker_list = list(set(final_ticker_list)) \n",
    "print(\"Fetching data...\")\n",
    "data = fetch_data(ticker_list)\n",
    "\n",
    "\n",
    "data.to_csv(\"raw_stock_data.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_combined_data(data):\n",
    "\n",
    "    label_encoder = LabelEncoder()\n",
    "    data['ticker_encoded'] = label_encoder.fit_transform(data['ticker'])\n",
    "\n",
    "    scalers = {}\n",
    "\n",
    "    sequences = []\n",
    "    targets = []\n",
    "    tickers = []\n",
    "\n",
    "    for ticker, group in data.groupby('ticker_encoded'):\n",
    "        \n",
    "        group = group.sort_values('Date')\n",
    "\n",
    "        scaler = MinMaxScaler()\n",
    "        scaled_features = scaler.fit_transform(group[feature_columns])\n",
    "        scalers[ticker] = scaler  \n",
    "\n",
    "        \n",
    "        for i in range(lookback, len(group)):\n",
    "            seq = scaled_features[i - lookback:i]\n",
    "            target = group['Close'].iloc[i]  \n",
    "            sequences.append(seq)\n",
    "            targets.append(target)\n",
    "            tickers.append(ticker)\n",
    "\n",
    "    X = np.array(sequences)\n",
    "    y = np.array(targets)\n",
    "\n",
    "   \n",
    "    ticker_to_int = {ticker: idx for idx, ticker in enumerate(label_encoder.classes_)}\n",
    "    \n",
    "    return X, y, ticker_to_int\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "processed_data = pd.DataFrame(X.reshape(-1, X.shape[2])) \n",
    "\n",
    "\n",
    "processed_data['target'] = np.repeat(y, lookback)  \n",
    "\n",
    "\n",
    "ticker_column = np.repeat(data['ticker_encoded'].values, lookback)\n",
    "if len(ticker_column) != len(processed_data):\n",
    "    ticker_column = ticker_column[:len(processed_data)]  \n",
    "\n",
    "processed_data['ticker'] = ticker_column\n",
    "\n",
    "\n",
    "processed_data.to_csv(\"processed_stock_data.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the LSTM model...\n",
      "Epoch 1/50\n",
      "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 144ms/step - loss: 87390.3984 - mae: 260.1103 - val_loss: 78346.5547 - val_mae: 246.3625\n",
      "Epoch 2/50\n",
      "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 137ms/step - loss: 82016.1641 - mae: 250.6907 - val_loss: 75479.5312 - val_mae: 240.4734\n",
      "Epoch 3/50\n",
      "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 137ms/step - loss: 78881.2812 - mae: 244.1500 - val_loss: 72722.4922 - val_mae: 234.6708\n",
      "Epoch 4/50\n",
      "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 141ms/step - loss: 76060.4766 - mae: 238.2814 - val_loss: 70109.7578 - val_mae: 229.0365\n",
      "Epoch 5/50\n",
      "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 139ms/step - loss: 72482.8516 - mae: 230.8539 - val_loss: 67610.4297 - val_mae: 223.5136\n",
      "Epoch 6/50\n",
      "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 153ms/step - loss: 71121.9141 - mae: 227.0246 - val_loss: 65229.7812 - val_mae: 218.1232\n",
      "Epoch 7/50\n",
      "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 140ms/step - loss: 71328.0625 - mae: 225.2085 - val_loss: 62926.6523 - val_mae: 212.7782\n",
      "Epoch 8/50\n",
      "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 136ms/step - loss: 67889.8828 - mae: 217.4529 - val_loss: 60718.8594 - val_mae: 207.5253\n",
      "Epoch 9/50\n",
      "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 159ms/step - loss: 65023.3086 - mae: 213.2835 - val_loss: 58588.1875 - val_mae: 202.3268\n",
      "Epoch 10/50\n",
      "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 241ms/step - loss: 62016.0273 - mae: 205.6652 - val_loss: 56535.7773 - val_mae: 197.1895\n",
      "Epoch 11/50\n",
      "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 248ms/step - loss: 59543.2461 - mae: 201.2214 - val_loss: 54552.8945 - val_mae: 192.0959\n",
      "Epoch 12/50\n",
      "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 234ms/step - loss: 59735.2773 - mae: 198.7428 - val_loss: 52647.3633 - val_mae: 187.0703\n",
      "Epoch 13/50\n",
      "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 250ms/step - loss: 56665.9023 - mae: 191.7949 - val_loss: 50814.5742 - val_mae: 182.1057\n",
      "Epoch 14/50\n",
      "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 168ms/step - loss: 53614.3086 - mae: 184.9228 - val_loss: 49048.0312 - val_mae: 177.1891\n",
      "Epoch 15/50\n",
      "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 142ms/step - loss: 52237.0000 - mae: 181.9061 - val_loss: 47343.4141 - val_mae: 172.3118\n",
      "Epoch 16/50\n",
      "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 130ms/step - loss: 49861.1094 - mae: 175.5081 - val_loss: 45699.7344 - val_mae: 167.4779\n",
      "Epoch 17/50\n",
      "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 129ms/step - loss: 50362.6211 - mae: 172.3705 - val_loss: 44119.0742 - val_mae: 162.7196\n",
      "Epoch 18/50\n",
      "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 129ms/step - loss: 47614.2539 - mae: 166.4352 - val_loss: 42601.4297 - val_mae: 158.0719\n",
      "Epoch 19/50\n",
      "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 129ms/step - loss: 46618.4766 - mae: 163.2060 - val_loss: 41141.8633 - val_mae: 153.5528\n",
      "Epoch 20/50\n",
      "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 142ms/step - loss: 45299.5625 - mae: 158.7669 - val_loss: 39739.3945 - val_mae: 149.1590\n",
      "Epoch 21/50\n",
      "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 135ms/step - loss: 42824.1680 - mae: 153.0723 - val_loss: 38387.9688 - val_mae: 144.8774\n",
      "Epoch 22/50\n",
      "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 144ms/step - loss: 41667.2578 - mae: 149.3247 - val_loss: 37100.5234 - val_mae: 140.7961\n",
      "Epoch 23/50\n",
      "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 141ms/step - loss: 40373.2266 - mae: 146.1164 - val_loss: 35860.1094 - val_mae: 136.8858\n",
      "Epoch 24/50\n",
      "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 139ms/step - loss: 40324.9023 - mae: 142.9659 - val_loss: 34674.7852 - val_mae: 133.2354\n",
      "Epoch 25/50\n",
      "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 144ms/step - loss: 37928.2188 - mae: 139.4718 - val_loss: 33549.0508 - val_mae: 129.8336\n",
      "Epoch 26/50\n",
      "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 182ms/step - loss: 36815.9688 - mae: 134.9745 - val_loss: 32466.3340 - val_mae: 126.5901\n",
      "Epoch 27/50\n",
      "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 167ms/step - loss: 36477.0820 - mae: 133.0084 - val_loss: 31432.2695 - val_mae: 123.4892\n",
      "Epoch 28/50\n",
      "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 149ms/step - loss: 35181.8867 - mae: 130.4802 - val_loss: 30442.9414 - val_mae: 120.5754\n",
      "Epoch 29/50\n",
      "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 155ms/step - loss: 33271.4570 - mae: 126.9290 - val_loss: 29506.6270 - val_mae: 117.8754\n",
      "Epoch 30/50\n",
      "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 156ms/step - loss: 31563.9863 - mae: 122.7246 - val_loss: 28610.7773 - val_mae: 115.3671\n",
      "Epoch 31/50\n",
      "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 152ms/step - loss: 32270.8047 - mae: 121.3958 - val_loss: 27763.8730 - val_mae: 113.0981\n",
      "Epoch 32/50\n",
      "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 159ms/step - loss: 29892.1855 - mae: 117.6654 - val_loss: 26962.3496 - val_mae: 111.0586\n",
      "Epoch 33/50\n",
      "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 167ms/step - loss: 29419.1465 - mae: 116.9620 - val_loss: 26199.7754 - val_mae: 109.1873\n",
      "Epoch 34/50\n",
      "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 153ms/step - loss: 29830.6133 - mae: 116.0045 - val_loss: 25479.6211 - val_mae: 107.4836\n",
      "Epoch 35/50\n",
      "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 160ms/step - loss: 29549.8691 - mae: 114.3709 - val_loss: 24802.9453 - val_mae: 105.9558\n",
      "Epoch 36/50\n",
      "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 159ms/step - loss: 29106.4102 - mae: 114.8928 - val_loss: 24159.5039 - val_mae: 104.5463\n",
      "Epoch 37/50\n",
      "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 159ms/step - loss: 26250.3848 - mae: 109.7622 - val_loss: 23561.9902 - val_mae: 103.2737\n",
      "Epoch 38/50\n",
      "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 161ms/step - loss: 27486.5098 - mae: 111.0625 - val_loss: 23000.7402 - val_mae: 102.1445\n",
      "Epoch 39/50\n",
      "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 158ms/step - loss: 25847.7148 - mae: 108.2983 - val_loss: 22417.7207 - val_mae: 101.0461\n",
      "Epoch 40/50\n",
      "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 181ms/step - loss: 25957.7207 - mae: 108.3468 - val_loss: 21890.6758 - val_mae: 100.1034\n",
      "Epoch 41/50\n",
      "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 168ms/step - loss: 23414.8555 - mae: 104.1206 - val_loss: 21417.9043 - val_mae: 99.2765\n",
      "Epoch 42/50\n",
      "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 163ms/step - loss: 25224.9570 - mae: 106.7482 - val_loss: 20983.7676 - val_mae: 98.5461\n",
      "Epoch 43/50\n",
      "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 174ms/step - loss: 24305.6055 - mae: 104.5281 - val_loss: 20584.1191 - val_mae: 97.8741\n",
      "Epoch 44/50\n",
      "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 167ms/step - loss: 22671.5078 - mae: 103.2603 - val_loss: 20223.1230 - val_mae: 97.2804\n",
      "Epoch 45/50\n",
      "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 169ms/step - loss: 23803.1816 - mae: 104.5727 - val_loss: 19894.5938 - val_mae: 96.7598\n",
      "Epoch 46/50\n",
      "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 170ms/step - loss: 23583.9668 - mae: 103.8766 - val_loss: 19599.6152 - val_mae: 96.3066\n",
      "Epoch 47/50\n",
      "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 164ms/step - loss: 22726.7520 - mae: 103.6922 - val_loss: 19330.4277 - val_mae: 95.9206\n",
      "Epoch 48/50\n",
      "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 170ms/step - loss: 22685.6602 - mae: 103.6868 - val_loss: 19087.1543 - val_mae: 95.5953\n",
      "Epoch 49/50\n",
      "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 168ms/step - loss: 22513.4785 - mae: 103.1942 - val_loss: 18872.5020 - val_mae: 95.3382\n",
      "Epoch 50/50\n",
      "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 163ms/step - loss: 21739.5684 - mae: 102.1116 - val_loss: 18681.3047 - val_mae: 95.1399\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved as 'lstm_stock_model.h5'\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "\n",
    "def create_lstm_model(input_shape):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(units=50, return_sequences=True, input_shape=input_shape))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(LSTM(units=50, return_sequences=True))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(LSTM(units=50, return_sequences=True))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(LSTM(units=50))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(units=1))\n",
    "    model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "    return model\n",
    "\n",
    "\n",
    "input_shape = (X_train.shape[1], X_train.shape[2])  \n",
    "model = create_lstm_model(input_shape)\n",
    "\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "print(\"Training the LSTM model...\")\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_test, y_test),\n",
    "    epochs=50,\n",
    "    batch_size=64,\n",
    "    callbacks=[early_stopping],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "\n",
    "model.save(\"lstm_stock_model.h5\")\n",
    "print(\"Model saved as 'lstm_stock_model.h5'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "loss, mae = model.evaluate(X_test, y_test, verbose=1)\n",
    "print(f\"Test Loss: {loss}, Test MAE: {mae}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import load_model\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Fetch the last 'lookback' days of data\n",
    "def fetch_recent_data(ticker, lookback=60):\n",
    "    stock_data = yf.download(ticker, period=\"90d\")  # Fetch the last 90 days for safety\n",
    "    stock_data = stock_data.tail(lookback)  # Get the last 'lookback' rows\n",
    "    return stock_data\n",
    "\n",
    "# Preprocess the data for prediction\n",
    "def preprocess_for_prediction(data, feature_columns, scaler):\n",
    "    scaled_features = scaler.transform(data[feature_columns])\n",
    "    return scaled_features\n",
    "\n",
    "# Load the trained model\n",
    "model = load_model(\"./lstm_stock_model.h5\")\n",
    "\n",
    "# Fetch recent data for COALINDIA.NS\n",
    "ticker = \"COALINDIA.NS\"\n",
    "recent_data = fetch_recent_data(ticker, lookback=lookback)\n",
    "\n",
    "# Ensure 'recent_data' contains all required feature columns\n",
    "if set(feature_columns).issubset(recent_data.columns):\n",
    "    # Initialize scaler for COALINDIA.NS\n",
    "    scaler = MinMaxScaler()\n",
    "    scaler.fit(recent_data[feature_columns])  # Refit scaler based on recent data\n",
    "    \n",
    "    # Prepare input sequence\n",
    "    input_sequence = preprocess_for_prediction(recent_data, feature_columns, scaler)\n",
    "    input_sequence = input_sequence.reshape(1, lookback, len(feature_columns))  # Shape (1, lookback, features)\n",
    "    \n",
    "    # Predict the next day's price\n",
    "    next_day_scaled = model.predict(input_sequence)[0][0]\n",
    "    \n",
    "    # Inverse scale to get the actual price\n",
    "    next_day_price = scaler.inverse_transform([[0, 0, 0, next_day_scaled, 0]])[0][3]  # Only inverse 'Close'\n",
    "    \n",
    "    print(f\"Predicted price for {ticker} on the next day: {next_day_price:.2f}\")\n",
    "else:\n",
    "    print(\"Error: Recent data is missing required feature columns.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
